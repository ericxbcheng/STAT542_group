---
title: 'STAT 542 Group Project: Skin Cancer Classification'
author: "Xianbin Cheng, Ziqin Xiong, Jingyu Li"
date: "December 16, 2019"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '80%',fig.show = "hold", fig.align = "center")
options(width = 90)
```

# Project Description and Summary

  Skin cancer, including melanoma and nonmelanoma skin cancer, is one of the most prevalent carcinomas worldwide. Clinal confirmatory test of skin cancer often requires procedure such as a biopsy, which is invasive and time-consuming. Building reliable models that can classify skin cancer based on dermoscopic images may extensively accelerate the diagnosis process and prevent the suffering of a biopsy. This dataset contains 300 images taken with different equipment with different magnification. They have different resolution; some appears to be redder and others look bluer; some have black frames and ruler on it and others don't, etc. Hence before training models, images were preprocessed with cropping, resizing, blurring, and dilation to eliminate irrelevant objects such as black frames, hair, bubble, and ruler on the lens. 
  
  For classification model, logistic regression with LASSO, support vector machine and random forest were used. Input of each image is the concatenation of RGB chanel of processed 300*225 image, so the total dimension is 202500. Each model were tuned for best performance. And the accuracies are around 70%. 
  
  With the goal of enhancing interpretability of the models, skin lesion was automatically identified by border detection and 22 features were engineered to reflect skin lesion’s shape characteristics (perimeter, area, border irregularity, shape compactness) and color characteristics (pixel mean and standard deviation of normalized RGB, HSV, CIE L*a*b color channels). Decision tree and logistic regression with LASSO model were then applied on these new features, which yield an overall accuracy of 60% and 74% respectively. These models have better interpretability and may facilitate rapid screening of skin cancer.

# Question 1

```{r, warning = FALSE, message = FALSE, eval = TRUE}
library(tidyverse)
library(imager)
library(imagerExtra)
library(MASS)
library(caret)
library(randomForest)
library(kernlab)
library(rpart)
library(rpart.plot)
library(glmnet)
library(ROCR)
library(kableExtra)
```



```{r, eval = TRUE}
#path for unprocessed images
files_benign = dir(path = "benign", full.names = TRUE)
files_malignant = dir(path = "malignant", full.names = TRUE)
```

## Data Processing

```{r, echo = FALSE, eval = TRUE}
# Cropping and resizing
my_crop = function(im, length, width){
  
  im_length = dim(im)[1]
  im_width = dim(im)[2]
  
  if(im_length == length & im_width == width){
    return(im)
  } else {
    if(im_length/im_width != length/width){
      
      im_length2 = length/width * im_width
      im2 = crop.borders(im = im, nx = (im_length - im_length2)/2)
      im3 = resize(im = im2, size_x = length, size_y = width, interpolation_type = 3)
      return(im3)
      
    } else {
      im2 = resize(im = im, size_x = length, size_y = width)
      return(im2)
    }
  }
}

# Remove black borders
rm_border = function(im, thresh_black, thresh_perc_x, thresh_perc_y){
  
  # Find out which pixels are black
  bool_black = grayscale(im) < thresh_black
  
  # Calculate how many black pixels per row/column. If the proportion of black pixels in a row/column exceeds thresh_perc, remove that row/column
  rm_pix_x = rowMeans(bool_black) > thresh_perc_x
  rm_pix_y = colMeans(bool_black) > thresh_perc_y
  return(crop.borders(im = im, nx = sum(rm_pix_x) / 2, ny = sum(rm_pix_y) / 2))
}
```

```{r, echo = FALSE}
# Preprocessing
for(i in 1:length(files_benign)){
  load.image(file = files_benign[i]) %>%
      rm_border(im = ., thresh_black = 0.1, thresh_perc_x = 0.2, thresh_perc_y = 0.5) %>%
      my_crop(im = ., length = 300, width = 225) %>%
      dilate_square(im = ., size = 3) %>%
      medianblur(im = ., n = 5) %>%
      save.image(im = ., file = paste0("my_benign/","benign", i, ".png"))
}

for(i in 1:length(files_malignant)){
  load.image(file = files_malignant[i]) %>%
      rm_border(im = ., thresh_black = 0.1, thresh_perc_x = 0.2, thresh_perc_y = 0.5) %>%
      my_crop(im = ., length = 300, width = 225) %>%
      dilate_square(im = ., size = 3) %>%
      medianblur(im = ., n = 5) %>%
      save.image(im = ., file = paste0("my_malignant/","malignant", i, ".png"))
}
```

```{r, eval = TRUE}
#path for processed images
files_benign2 = dir(path = "my_benign", full.names = TRUE)
files_malignant2 = dir(path = "my_malignant", full.names = TRUE)
```

```{r, echo = FALSE, eval = FALSE}
# pix to matrix
pix2vec = function(im){
  c(as.vector(R(im)), as.vector(G(im)), as.vector(B(im)))
}

# Convert processed images to matrices
pic_all = lapply(X = c(files_benign2, files_malignant2), FUN = load.image) %>%
  sapply(X = ., FUN = pix2vec) %>%
  t() 

saveRDS(object = pic_all, file = "processed.rds")
```

  The raw data are consisted of 300 color photos of different sizes. As the photos were taken with a dermoscope on skin with a variety of conditions, the goal of photo preprocessing is to eliminate irrelevant objects in the view, such as the black frame, the dark area beyond the ocular lens, the hair, the bubbles, and the ruler on the object lens. To start with, black frames and dark areas were identified by converting photos to grayscale and checking the pixels in each row/column. A row was considered a black frame when over 50% of the pixels were black (normalized pixel value < 0.1) while a column was considered intersecting a dark area when over 20% of the pixels were black (Celebi *et al.*, 2008). These rows and columns were cropped from the original photo. Next, all the photos were further cropped and resized to 300 x 225 pixels with linear interpolation to reduce computation time. To remove hair and the ruler, image dilation was conducted with a size-3 square structuring element (Do Hyun Chung & Sapiro, 2000). Furthermore, bubbles were blurred by applying a 5 x 5 median filter (Masood & Al-Jumaily, 2013). Eventually, each photo was converted into a vector of 202,500 pixels, with pixels in R, G, B channels concatenating one another sequentially. An example is provided here to demonstrate the preprocessing effect (**Fig.1**, **Fig.2**).
  
```{r, echo = FALSE, eval = TRUE, fig.keep = "hold", fig.height= 5,fig.width= 10, out.width = "80%"}
# Demo for image prepocessing
par(mfrow = c(1,2),mar = c(5,4,6,2))
load.image(file = str_subset(string = files_benign, pattern = "0210")) %>% plot(main = "Fig.1 An example image before preprocessing.")
load.image(file = str_subset(string = files_benign2, pattern = "benign24")) %>% plot(main = "Fig.2 An example image after preprocessing.")

```

## Classification Models Based On Pixels

```{r, eval = TRUE}
pic_all = readRDS(file = "processed.rds")
colnames(pic_all) = str_c(rep("X", times = ncol(pic_all)), seq_len(length.out = ncol(pic_all)))
y = rep(x = c("b","m"), each = 150)

# train-test split
set.seed(123)
trn_idx = createDataPartition(y = y, p = 0.7, list = FALSE)
trn_data = pic_all[trn_idx, ] %>% 
  scale(x = ., center = TRUE, scale = TRUE)
trn_y = y[trn_idx]

tst_data = pic_all[-trn_idx, ] %>%
  scale(x = ., center = TRUE, scale = TRUE)
tst_y = y[-trn_idx]
rm(pic_all)
```



To train the models, 70% of the data were treated as the training set and 30% of the data were treated as the testing set. As a preprocessing step, the data were scaled by centering and scaling. The seed was set at 123 to ensure reproducibility. 

### Logisitic Regression with LASSO

```{r}
# Lasso GLM
set.seed(123)
fit.glm = cv.glmnet(trn_data, trn_y, family = "binomial", alpha = 1)
saveRDS(fit.glm,file = "Q1_glm.rds")
```

```{r, eval = TRUE}
# Lasso GLM
fit.glm = readRDS(file = "Q1_glm.rds")
pred.glm = factor(predict(fit.glm, tst_data, "lambda.min", type = "class"))
result_lasso = confusionMatrix(as.factor(tst_y), pred.glm, positive = 'm')

# Non zero coefficient
nonzeroCoef = (rownames(coef(fit.glm, s = 'lambda.min'))[coef(fit.glm, s = 'lambda.min')[,1] != 0])[-1] 
nonzeroCoef = map_dbl(nonzeroCoef, ~ as.numeric(unlist(strsplit(.x, split='X', fixed=TRUE))[2]))
num_R = sum(nonzeroCoef <= 300 * 225 )
num_G = sum(nonzeroCoef <= (300 * 225 *2)) - num_R
num_B = length(nonzeroCoef) - num_R - num_G
num_df = cbind(Red = num_R, Green = num_G, Blue = num_B)

# ROC 
pred.roc = prediction(predict(fit.glm, tst_data, "lambda.min"), labels = tst_y)
perf = performance(pred.roc,"tpr","fpr")
```

  We used 10-fold cross validation with logistic regression and set “m” (Malignant) as the positive level. We let the function choose and tune the best tuning parameter, `lambda`. The `lambda.min` which achieves the minimum mean cross-validated error is `r fit.glm$lambda.min`, and we chose this lambda to validate the model (**Fig.4**). 

 Applying the model associated with the lambda we chose on the testing data, the accuracy of predicting the tumor to be benign and malignant achieved a moderately high accuracy,`r result_lasso$overall["Accuracy"]`, indicating that the logistic regression model was well-performed at identifying tumors. 
 Meanwhile, the sensitivity was `r result_lasso$byClass["Sensitivity"]`, which was still moderately high, and it indicated this model was moderately effective in identifying malignant tumors. The specificity, `r result_lasso$byClass["Specificity"]` was similar to the sensitivity, and indicated this model was also moderately effective in identifying benign tumors. The ROC plot also demonstrated the relationship between sensitivity and specificity. We could see from the plot that both of them could achieve a relative high level at the same time, when both were around 0.7 ~ 0.75 (**Fig.5**).

 Moving deeper, we found with lasso penalty, there were in total 33 pixels that had non-zero coefficients, indicating they were contributing to the final classification. 3 of them were from Red in RGB channels, 7 of them were from Green, and 23 of them were from Blue (**Table 1**). Just from the number of non-zero coefficients in the model, we assumed the color blue in each pixel would be more significant than others when identifying tumors. However, the number of color channel with nonzero coefficients couldn’t tell much information about the relationship between pixel RGB and the tumor, so we need more intuitive features to identify them.
 
\begin{table}[ht]
\centering
\begin{tabular}{llll}
\hline
\textbf{}                          & \textbf{Red} & \textbf{Green} & \textbf{Blue} \\
\hline
\textbf{\# of contributing pixels} & 7            & 5              & 30        \\   
\hline
\end{tabular}
\caption{number of contributing pixels in different channels}
\end{table}
 
 
```{r, eval = TRUE,fig.keep = "hold", fig.height= 5,fig.width= 10, out.width = "80%"}
# log lambda vs. coefficients
par(mfrow = c(1,2),mar = c(5,4,6,2))
plot(fit.glm$glmnet.fit, "lambda")
title("Fig.4 Plot of lambda versus coefficients.", line = 3)
abline(v = log10(fit.glm$lambda.min), col = "red")

# ROC plot
plot(perf, colorize=TRUE, main = "Fig.5 ROC plot of the LASSO GLM model.")
```

### SVM with Radial Basis Function

```{r, eval = FALSE, echo = FALSE}
# SVM
set.seed(123)
mod_svm = train(x = trn_data, 
                y = as.factor(trn_y), method = "svmRadial", preProcess = NULL,
                metric = "ROC", trControl = trainControl(
                  method = "cv", number = 10, classProbs = TRUE, 
                  summaryFunction = twoClassSummary))
saveRDS(mod_svm,file = "mod_svmRadial_roc_cv_10.rds")
```

```{r, echo = FALSE, eval = TRUE}
# SVM with radial basis kernel
mod_svm = readRDS(file = "mod_svmRadial_roc_cv_10.rds")
pred_svm = predict(object = mod_svm$finalModel, newdata = as.data.frame(tst_data))
result_svm = confusionMatrix(data = as.factor(pred_svm), reference = as.factor(tst_y), positive = "m")
```

  A SVM model was trained with the radial basis function kernel. The parameter "cost" was tuned over 0.25, 0.5, 1 by 10-fold cross-validation and the model performance was evaluated by ROC. The tuning result suggested that the cost should be set at 1 as the cross-validation ROC was maximized at this point. 
  
  The final SVM model with cost = 1 could achieve a relatively high test accuracy of `r result_svm$overall["Accuracy"]`. Specifically, this model had a sensitivity of `r result_svm$byClass["Sensitivity"]` and a specificity of `r result_svm$byClass["Specificity"]`, which indicates that the model would perform better at predicting a malignant melanoma than predicting a benign melanoma. On one hand, this model would be useful in malignant cancer screening as it has a lower false negative rate. On the other hand, its high false positive rate could impose a noticeable financial burden on patients who may have to go through unnecessary sequential testing.

```{r, eval = TRUE, out.width = "50%"}
plot(mod_svm, main = "Fig.6 Parameter tuning for SVM model")
```

### Random Forest

We also fitted random forest models. Here we used Python to fit the model in consideration of efficiency. Since the input here has extremely large dimension (p = 202500), the key parameters to tune are mtry (max parameter to consider we split each tree) and max depth of each tree (max node = $2^{\text{max depth}}$). We first tune the max depths when fixing mtry to be 1, i.e., we choose all features and no bootstrap is performed. We can see below that when max tree depth is 4 (max node size 16), the accuracy reach maximum of 0.7. (when training a random forests of 100 trees)

\begin{center}
\includegraphics[width=0.8\textwidth,height=\textheight]{depth_vs_accu2.jpg}
\end{center}

We then tune the mtry parameter: fixing tree depth to be 4 and train model with 20%, 40%, 60%, 80%, 100% features to consider when split the tree. We can see below that when we consider all features, the accuracy is highest. But the difference is small, considering that the randomness is still very high when we have 202500 dimensions and fit only 100 trees.

\begin{center}
\includegraphics[width=0.8\textwidth,height=\textheight]{mtry_vs_accu.jpg}
\end{center}

The performance of this final forest is:



\begin{table}[ht]
\centering
\begin{tabular}{lllll}
\hline
\textbf{F1 Score}                     & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{Precision} & \textbf{F1 Score} \\ \hline
\hline
\textbf{Final Random Forest Model} & 0.7               & 0.7                  & 0.7                & 0.7               \\ \hline
\end{tabular}
\end{table}

And below is a summary of Question 1. 

```{r, eval = TRUE}
Q1_table = tibble(Model = c("Logistic Regression with Lasso", "SVM with radial basis kernel", "Random Forest"), 
                  Parameters = c(paste0("lambda = ", round(x = fit.glm$lambda.min, digits = 3)), 
                                 "sigma = 4.6e-06, C = 1", 
                                 c("max.tree.depth = 4, n.trees = 100, mtry = 202500")),
                  Test_Accuracy = c(result_lasso$overall["Accuracy"], result_svm$overall["Accuracy"], 0.7),
                  Sensitivity = c(result_lasso$byClass["Sensitivity"], result_svm$byClass["Sensitivity"], 0.7),
                  Specificity = c(result_lasso$byClass["Specificity"], result_svm$byClass["Specificity"], 0.698))
# Table 2
kable_styling(kable_input = kable(x = Q1_table, format = "latex", caption = "Summary of models in question 1."), 
              full_width = TRUE, latex_options = "hold_position")
```

# Question 2

## Literature Review

  Among all the clinical diagnostic criteria proposed in the literature, the “ABCD” criteria stand out as the most widely-adopted method. The acronym stands for asymmetry, border irregularity, color variegation, and diameter > 6 mm. Alternative criteria like the Glasgow 7-point checklist also mentions relevant characteristics such as varying size, shape, color (blue-white veil structure) and the observation of bleeding and inflammation. (Rigel, Russak, & Friedman, 2010). It has also been reported that features such as vascular growth, thickness of the lesion, evolving lesion size are associated with higher risk of developing skin cancer (Martinez & Otley, 2001). Therefore, the shape and color of skin cancer may be crucial to classification. 

## Feature Engineering

```{r, eval = TRUE}
# Return a pixel set of region of interest (TRUE means background, FALSE means tumor)
get_ROI = function(im, thr){
  
  a = im %>%
    grayscale(.) %>%
    threshold(im = ., thr = thr)
  
  return(a)
}

calc_feature_shape = function(bool){
  
  # perimeter
  p = boundary(bool) %>% sum()
  
  # Area
  A = sum(!bool)
  
  # Irregularity index I = p^2/(4*pi*A)
  irr = p ^ 2 / (4 * pi * A)
  
  # Compactness C = A_object / A_circle_w_same_perimeter
  comp = A / (p ^ 2 / (4 * pi))
  
  return(c(perimeter = p, area = A, irregularity = irr, compactness = comp))
}


# Border detection
show_border = function(im, bool){
  
  border = boundary(bool)
  im[border] = 1
  return(im)
}

# Extract ROI (color may change, not useful)
crop_ROI = function(im, bool){
  im[bool] = 1
  return(im)
}
```

```{r, echo = FALSE, eval = TRUE}
# color features: mean and standard devs of each color channel
get_mean_sd = function(im, bool){
  
  # Split an image by color channels
  a = imsplit(im = im, axis = "c")
  
  # Calculate means and sds of the ROI (Remember bool = FALSE when it's the tumor)
  means = map_dbl(.x = a, .f = function(x){mean(x[!bool])})
  sds = map_dbl(.x = a, .f = function(x){sd(x[!bool])})
  
  return(list(means = means, sds = sds))
}

calc_feature_color = function(im, bool){
  
  # RGB
  RGB = unlist(get_mean_sd(im = im, bool = bool))
  names(RGB) = str_c(rep(c("mean.", "sd."), each = 3), rep(c("R", "G", "B"), times = 2), sep = "")
  
  # HSV
  HSV = unlist(get_mean_sd(im = RGBtoHSV(im = im), bool = bool))
  names(HSV) = str_c(rep(c("mean.", "sd."), each = 3), rep(c("H", "S", "V"), times = 2), sep = "")
  
  # Lab
  Lab = unlist(get_mean_sd(im = RGBtoLab(im = im), bool = bool))
  names(Lab) = str_c(rep(c("mean.", "sd."), each = 3), rep(c("L", "a", "b"), times = 2), sep = "")
  
  return(c(RGB, HSV, Lab))
}

# Wrapper for shape and color features
calc_feature_all = function(im, thr){
  
  ROI = get_ROI(im = im, thr = thr)
  
  shape = calc_feature_shape(bool = ROI)
  color = calc_feature_color(im = im, bool = ROI)
  
  return(c(shape, color))
}
```

```{r, echo = FALSE, eval = FALSE}
# Create a new dataset with new features
data2 = map(.x = c(files_benign2, files_malignant2), .f = function(x){calc_feature_all(im = load.image(x), thr = "auto")}) %>%
  rbind_list() %>%
  cbind.data.frame(y, .)
```

  Based on the literature review, several shape and color features have been engineered with goal of capturing the essence of each skin cancer image. 

  Before developing features, the skin lesion or the region of interest (ROI) was identified and the border of skin lesion was detected. Specifically, each image was converted to a grayscale image, which was then binarized by an automatically determined threshold. A grayscale pixel was converted to 1 if it exceeded the threshold and 0 if otherwise. This resulted in a black-and-white image where the black region represented the ROI and the white represented the background. An example of border detection is shown in **Fig.7**. 

  Shape features such as perimeter and area were first engineered. Perimeter (P) of an ROI was defined as the number of pixels on the border and area (A) was defined as the number of pixels covering the entire ROI. On top of these 2 basic shape features, irregularity index ($I=\frac {P^2}{4\pi A}$) and compactness (the ratio between the area of the ROI and the area of a circle with the same perimeter as the ROI, $C = \frac {4\pi A}{P^2}$) were constructed (Celebi *et al.*, 2007; Golston, Stoecker, Moss, & Dhillon, 1992). 

  Color features such as the mean and standard deviation of pixels in each color channel were constructed. Generally, each image was split by its 3 color channels and the pixels within the ROI were extracted and subjected to calculation of the mean and the standard deviation. To create color features possessing such characteristics as invariance to illumination intensity, perceptual uniformity, and decoupling of chrominance and luminance, we have chosen 3 different color spaces (normalized RGB, HSV, CIE Lab) to calculate the means and standard deviations (Celebi *et al.*, 2007; Green, Martin, Pfitzner, O’Rourke, & Knight, 1994). 

  In summary, a total of 22 features were engineered for each image including 4 shape features and 18 color features.

```{r, eval = TRUE, out.width = "50%"}
im_demo = load.image(file = str_subset(string = files_benign2, pattern = "108"))
ROI = get_ROI(im = im_demo, thr = "auto") 
show_border(im = im_demo, bool = ROI) %>% plot(main = "Fig.7 Demonstration of border detection.")
```

## Classification Models Based On New Features

```{r, eval = TRUE}
data2 = read.csv(file = "feature_engineering.csv", header = TRUE)
trn_data2 = data2[trn_idx, ]
tst_data2 = data2[-trn_idx, ]
```

### Tree Model

```{r, echo = FALSE, eval = TRUE, cache = TRUE}
# Tree
set.seed(123)
mod_cart = train(form = as.factor(y) ~ ., 
                 data = trn_data2,
                  method = "rpart", 
                  preProcess = NULL, 
                  metric = "ROC",
                  trControl = trainControl(
                    method = "cv", number = 5, classProbs = TRUE, 
                    summaryFunction = twoClassSummary), 
                 tuneGrid = expand.grid(cp = c(0, 0.001, 0.005, 0.01, 0.05)))
```

```{r, eval = TRUE}
pred_cart = predict(object = mod_cart, newdata = tst_data2)
result_cart = confusionMatrix(data = pred_cart, reference = as.factor(tst_y), positive = "m")
```

  A single tree model was trained with the complexity parameter as the tuning parameter. 5-fold cross-validation was used to tune over such complexity parameter values as 0, 0.001, 0.005, 0.01, and 0.05. The cross validation result shows that the ROC was maximized when `cp` = `r mod_cart$bestTune` and thus this was chosen for the final tree model (**Fig.8**). The final model has an overall accuracy of `r result_cart$overall["Accuracy"]` with the sensitivity being `r result_cart$byClass["Sensitivity"]` and the specificity being `r result_cart$byClass["Specificity"]`. Evidently, this model has a probability of 62% to correctly classify a malignant mole at the cost of producing many false positives with a probability of 42%.
  
  A major advantage of using a tree model is that the insights on how classification is performed can be revealed by analyzing the decision tree (**Fig.9**). Clearly, perimeter played a major role in classification where a perimeter >= 662 pixels might lead to higher probability (69%) of having a malignant skin cancer. This is consistent with the “ABCD” rule in the literature that associates malignancy with a larger diameter. Further down the branch on the right, the mean values of the normalized blue and the green channel as well as the irregularity index were crucial for classification. Generally, a mole is classified as malignant with a probability of at least 71% when its image’s mean value of normalized blue channel is < 0.5 (which means darker in blue), its mean value of normalized green channel is >= 0.6 (which means brighter in green), and its irregularity index is >= 5.2. These decision rules are also consistent with the “ABCD” rule that color variegation, blue-white veil, and irregular border may indicate malignancy (Rigel et al., 2010). With the branch on the left, the standard deviations of hue and value were the driving forces for classification. Evidently, larger standard deviations of hue (>= 96) and value (>= 0.097) may lead to classifying a model as malignant with a probability of 69%. 

  In summary, our tree model may be beneficial to doctors who need assistance with rapid screening for malignant skin cancer as it provides easily-understandable decision rules and relatively satisfactory accuracy.  

```{r, eval = TRUE, fig.keep = "hold", fig.height= 5,fig.width= 10, out.width = "80%"}
plot(mod_cart, main = "Fig.8 Cross-validation of the tree model.")
rpart.plot(x = mod_cart$finalModel, main = "Fig.9 The decision tree.")
```


### Logistic Regression with LASSO

```{r, eval = TRUE, echo = FALSE, cache = TRUE}
# logistic regression (lasso)
feature_trn.x = as.matrix(data2[trn_idx, -1])
feature_trn.y = as.matrix(data2[trn_idx, 1])
feature_tst.x = as.matrix(data2[-trn_idx, -1])
feature_tst.y = as.matrix(data2[-trn_idx, 1])

set.seed(123)
feature.fit.glm = cv.glmnet(feature_trn.x, feature_trn.y, family = "binomial", alpha = 1)
feature.pred.glm = factor(predict(feature.fit.glm, feature_tst.x, "lambda.min", type = "class"))
result_lasso_feature = confusionMatrix(as.factor(feature_tst.y), feature.pred.glm, positive = 'm')

# ROC 
feature.pred.roc = prediction(predict(feature.fit.glm, feature_tst.x, "lambda.min"), labels = feature_tst.y)
feature.perf = performance(feature.pred.roc,"tpr","fpr")

# Non zero coefficient
nonzeroCoef = (rownames(coef(feature.fit.glm, s = 'lambda.min'))[coef(feature.fit.glm, s = 'lambda.min')[,1] != 0])[-1] 
df_coef = coef(feature.fit.glm, s = 'lambda.min')[as.numeric(which(coef(feature.fit.glm, s = 'lambda.min')[,1] != 0)),]
```


 We again used 10-fold cross validation with logistic regression and set “m” (Malignant) as the positive level. We again let the function choose and tune the best tuning parameter, lambda. The lambda.min which achieves the minimum cross-validated error is `r fit.glm$lambda.min`, and we chose this lambda to validate the model (**Fig.10**). 

 The accuracy of predicting the tumor for the testing data was `r result_lasso_feature$overall["Accuracy"]`, a little higher than that in Question 1. Both sensitivity and specificity increased compared to those in Question 1. The ROC plot also showed a more desired curve indicating the sensitivity and specificity. By looking at these statistics, we assumed that the feature engineering process successfully helped increase the accuracy of identifying tumors (**Fig.11**). 
 Lasso helped us to do variable selection. Under the lambda we chose, only 9 out of the 22 features had nonzero coefficients and thus contributed to the final identification. They are "perimeter”, ”area”, “compactness”, “mean.B”, “sd.B”, ”mean.H”, "sd.H”, “sd.S” and “sd.V”. Then we took a deeper look at their individual coefficients. We surprisedly found that the coefficients of mean.B (mean value of blue channel) and sd.S (standard deviation of saturation) had much more lower coefficients than any one else, and both coefficients were negative. This meant, when the mean value of the blue channel  or the standard deviation of the saturation value of a mole was high, then the mole was much more likely to be a benign one. The third feature with relatively significant coefficient was compactness (ratio between the area of the mole and the area of the circle with the same parimeter), with coefficient about -3, which indicated that higher compactness of the mole contributed to a higher probability of being benign.

```{r, eval = TRUE, fig.keep = "hold", fig.height= 5,fig.width= 10, out.width = "80%"}
# Nonzero
kable_styling(kable(df_coef, col.names = "Coefficient", format = "latex", caption = "Nonzero coefficients of the logistic regression with LASSO model.",booktabs = T), position = "center", latex_options = "hold_position")

# log lambda vs. coefficients
par(mfrow = c(1,2),mar = c(5,4,6,2))
plot(feature.fit.glm$glmnet.fit, "lambda", main = "Fig.10 Coefficients VS lambda.")
abline(v = log10(feature.fit.glm$lambda.min), lty = "dashed")

# ROC
plot(feature.perf, colorize=TRUE, main = "Fig.11 ROC curve of the logistic regression with LASSO model.")
```



# References

Celebi, M. E., Kingravi, H. A., Iyatomi, H., Aslandogan, Y. A., Stoecker, W. V., Moss, R. H., … Menzies, S. W. (2008). Border detection in dermoscopy images using statistical region merging. Skin Research and Technology: Official Journal of International Society for Bioengineering and the Skin (ISBS) [and] International Society for Digital Imaging of Skin (ISDIS) [and] International Society for Skin Imaging (ISSI), 14(3), 347–353. https://doi.org/10.1111/j.1600-0846.2008.00301.x

Celebi, M. E., Kingravi, H. A., Uddin, B., Iyatomi, H., Aslandogan, Y. A., Stoecker, W. V., & Moss, R. H. (2007). A methodological approach to the classification of dermoscopy images. Computerized Medical Imaging and Graphics, 31(6), 362–373. https://doi.org/10.1016/j.compmedimag.2007.01.003

Do Hyun Chung, & Sapiro, G. (2000). Segmenting skin lesions with partial-differential-equations-based image processing algorithms. IEEE Transactions on Medical Imaging, 19(7), 763–767. https://doi.org/10.1109/42.875204

Golston, J. E., Stoecker, W. V., Moss, R. H., & Dhillon, I. P. S. (1992). Automatic detection of irregular borders in melanoma and other skin tumors. Computerized Medical Imaging and Graphics, 16(3), 199–203. https://doi.org/10.1016/0895-6111(92)90074-J

Green, A., Martin, N., Pfitzner, J., O’Rourke, M., & Knight, N. (1994). Computer image analysis in the diagnosis of melanoma. Journal of the American Academy of Dermatology, 31(6), 958–964. https://doi.org/10.1016/S0190-9622(94)70264-0

Martinez, J.-C., & Otley, C. C. (2001). The Management of Melanoma and Nonmelanoma Skin Cancer: A Review for the Primary Care Physician. Mayo Clinic Proceedings, 76(12), 1253–1265. https://doi.org/10.4065/76.12.1253

Masood, A., & Al-Jumaily, A. A. (2013). Computer Aided Diagnostic Support System for Skin Cancer: A Review of Techniques and Algorithms. International Journal of Biomedical Imaging, 2013. https://doi.org/10.1155/2013/323268

Rigel, D. S., Russak, J., & Friedman, R. (2010). The evolution of melanoma diagnosis: 25 years beyond the ABCDs. CA: A Cancer Journal for Clinicians, 60(5), 301–316. https://doi.org/10.3322/caac.20074

